{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 5, Part i: Reinforcement Learning DEMO\n"]},{"cell_type":"markdown","metadata":{},"source":["## Reinforcement Learning Example\n","\n","In this example from Reinforcement Learning, the task is to use tools from Machine Learning to predict how an agent should act. We will then use those predictions to drive the behavior of the agent. Ideally, our intelligent agent should get a much better score than a random agent.\n","\n","## Key concepts:\n","\n","- **Observation**: These are the states of the game. It describes where the agent currently is.\n","- **Action**: These are the moves that the agent makes.\n","- **Episode**: One full game played from beginning (`env.reset()`) to end (when `done == True`).\n","- **Step**: Part of a game that includes one action. The game transitions from one observation to the next.\n","\n","## Setup\n","\n","This exaple uses the Python library [OpenAI Gym](https://gym.openai.com/docs/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01).\n","\n","If you want to install everything (gym can run atari games.) follow [these instructions](https://github.com/openai/gym#installing-everything).\n","\n","Now we can build an environment using OpenAI. \n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gym\n","  Downloading gym-0.26.2.tar.gz (721 kB)\n","     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n","      --------------------------------------- 10.2/721.7 kB ? eta -:--:--\n","      --------------------------------------- 10.2/721.7 kB ? eta -:--:--\n","     - ----------------------------------- 30.7/721.7 kB 187.9 kB/s eta 0:00:04\n","     -- ---------------------------------- 41.0/721.7 kB 217.9 kB/s eta 0:00:04\n","     --- --------------------------------- 71.7/721.7 kB 302.7 kB/s eta 0:00:03\n","     ------- ---------------------------- 153.6/721.7 kB 573.4 kB/s eta 0:00:01\n","     ----------- ------------------------ 225.3/721.7 kB 765.3 kB/s eta 0:00:01\n","     --------------- -------------------- 307.2/721.7 kB 905.4 kB/s eta 0:00:01\n","     ------------------- ---------------- 389.1/721.7 kB 969.0 kB/s eta 0:00:01\n","     -------------------------- ----------- 512.0/721.7 kB 1.1 MB/s eta 0:00:01\n","     -------------------------------- ----- 624.6/721.7 kB 1.3 MB/s eta 0:00:01\n","     -------------------------------------  716.8/721.7 kB 1.3 MB/s eta 0:00:01\n","     -------------------------------------- 721.7/721.7 kB 1.2 MB/s eta 0:00:00\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n","Requirement already satisfied: numpy>=1.18.0 in c:\\users\\kymno\\anaconda3\\lib\\site-packages (from gym) (1.24.3)\n","Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\kymno\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n","Collecting gym-notices>=0.0.4 (from gym)\n","  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (pyproject.toml): started\n","  Building wheel for gym (pyproject.toml): finished with status 'done'\n","  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827631 sha256=f690975074201d8cd16004646ee06aef3d1602a6e5e180f116e5f89f81b3fed2\n","  Stored in directory: c:\\users\\kymno\\appdata\\local\\pip\\cache\\wheels\\1c\\77\\9e\\9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n","Successfully built gym\n","Installing collected packages: gym-notices, gym\n","Successfully installed gym-0.26.2 gym-notices-0.0.8\n"]}],"source":["#!mamba install -qy -c conda-forge gym==0.21.0\n","!pip install gym"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import gym\n","import pandas\n","import numpy as np\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","def warn(*args, **kwargs):\n","    return None\n","\n","warnings.warn = warn"]},{"cell_type":"markdown","metadata":{},"source":["# The first part of the game uses the environment FrozenLake-V0\n","\n","This is a small world with 16 tiles. \n","\n","    SFFF\n","    FHFH\n","    FFFH\n","    HFFG\n","\n","The game starts at the S tile. The object of the game is to get to the goal (G) without landing in a hole (H).\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(0, {'prob': 1})\n"]}],"source":["# Build an environment with gym.make()\n","env = gym.make('FrozenLake-v1') # Build a fresh environment\n","\n","# Start a new game with env.reset()\n","current_observation = env.reset() # This starts a new \"episode\" and returns the initial observation\n","\n","# The current observation is just the current location\n","print(current_observation) # Observations are just a number"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;31mType:\u001b[0m           OrderEnforcing\n","\u001b[1;31mString form:\u001b[0m    <OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>\n","\u001b[1;31mFile:\u001b[0m           c:\\users\\kymno\\anaconda3\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py\n","\u001b[1;31mDocstring:\u001b[0m     \n","A wrapper that will produce an error if :meth:`step` is called before an initial :meth:`reset`.\n","\n","Example:\n","    >>> from gym.envs.classic_control import CartPoleEnv\n","    >>> env = CartPoleEnv()\n","    >>> env = OrderEnforcing(env)\n","    >>> env.step(0)\n","    ResetNeeded: Cannot call env.step() before calling env.reset()\n","    >>> env.render()\n","    ResetNeeded: Cannot call env.render() before calling env.reset()\n","    >>> env.reset()\n","    >>> env.render()\n","    >>> env.step(0)\n","\u001b[1;31mInit docstring:\u001b[0m\n","A wrapper that will produce an error if :meth:`step` is called before an initial :meth:`reset`.\n","\n","Args:\n","    env: The environment to wrap\n","    disable_render_order_enforcing: If to disable render order enforcing"]}],"source":["env.env?"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# we can print the environment if we want to look at it\n","env.render() "]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["our action space: Discrete(4)\n","our new action: 3\n"]}],"source":["# the action space for this environment includes four discrete actions\n","\n","print(f\"our action space: {env.action_space}\")\n","\n","new_action = env.action_space.sample() # we can randomly sample actions\n","\n","print(f\"our new action: {new_action}\") # run this cell a few times to get an idea of the action space\n","# what does it look like?"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["observation: 5, reward: 0, done: True, info: False\n"]}],"source":["# now we act! do this with the step function\n","\n","new_action = env.action_space.sample()\n","\n","observation, reward, done, info, probs = env.step(new_action)\n","\n","# here's a look at what we get back\n","print(f\"observation: {observation}, reward: {reward}, done: {done}, info: {info}\")\n","\n","env.render() "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["observation: 0, reward: 0.0, done: False, info: False and probs {'prob': 0.3333333333333333}\n","observation: 4, reward: 0.0, done: False, info: False and probs {'prob': 0.3333333333333333}\n","observation: 4, reward: 0.0, done: False, info: False and probs {'prob': 0.3333333333333333}\n","observation: 8, reward: 0.0, done: False, info: False and probs {'prob': 0.3333333333333333}\n","observation: 4, reward: 0.0, done: False, info: False and probs {'prob': 0.3333333333333333}\n"]}],"source":["# we can put this process into a for-loop and see how the game progresses\n","\n","current_observation = env.reset() # start a new game\n","\n","for i in range(5): # run 5 moves\n","\n","    new_action = env.action_space.sample() # same a new action\n","\n","    observation, reward, done, info, probs1 = env.step(new_action) # step through the action and get the outputs\n","\n","    # here's a look at what we get back\n","    print(f\"observation: {observation}, reward: {reward}, done: {done}, info: {info} and probs {probs1}\")\n","\n","    env.render() "]},{"cell_type":"markdown","metadata":{},"source":["Now we can guess what each of the outputs mean. \n","\n","**Observation** refers to the number of the tile. The tiles appear to be numbered\n","\n","    0 1 2 3\n","    4 5 ...\n","    \n","**Reward** refers to the outcome of the game. We get 1 if we win, zero otherwise.\n","\n","**Done** tells us if the game is still going. It goes to true when we win or fall into a hole.\n","\n","**info** gives extra info about the world. Here, it's probabilities. Can you guess what this means here? Perhaps the world is a bit noisy.\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["action:2 observation: 0, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:2 observation: 4, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:0 observation: 4, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:3 observation: 4, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:3 observation: 0, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:0 observation: 0, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:3 observation: 1, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:3 observation: 1, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:3 observation: 1, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:0 observation: 0, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:2 observation: 4, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:0 observation: 8, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:2 observation: 4, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:2 observation: 8, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:1 observation: 9, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:1 observation: 13, reward: 0.0, done: False, info: False - probs {'prob': 0.3333333333333333}\n","action:0 observation: 12, reward: 0.0, done: True, info: False - probs {'prob': 0.3333333333333333}\n"]}],"source":["# Here's how to simulate an entire episode\n","# We're going to stop rendering it every time to save space\n","# try running this a few. Does it ever win?\n","\n","current_observation = env.reset()\n","done = False\n","\n","while not done:    \n","    new_action = env.action_space.sample()\n","    new_observation, reward, done, info, probs = env.step(new_action)\n","    print(f\"action:{new_action} observation: {new_observation}, reward: {reward}, done: {done}, info: {info} - probs {probs}\")"]},{"cell_type":"markdown","metadata":{},"source":["Things to think about:\n","- What things do you notice about how the environment and actions work?\n","- What do you think the actions mean?\n","- When the agent performs the same action from the same place (same observation), does the same outcome happen every time?\n"]},{"cell_type":"markdown","metadata":{},"source":["The environment has some squares that always end the game (`H` in the render), some that don't (`F`), and one that is presumably the reward, if you get to it.\n","\n","The actions seem like up, down, left, and right. But they also seem stochastic. There seems to be a 1/3 chance of going into 3 different squares with each action. \n"]},{"cell_type":"markdown","metadata":{},"source":["# Part 1: Gather data\n","\n","We want to build an intelligent actor but first we have to gather data on which actions are useful.\n","\n","Use the code above as reference. Run a *random* agent through 1,000 or more episodes and collect data on each step.\n","\n","I recommend you store this data in a pandas dataframe. Each row should be a step. Your features should include the following features or similar \n","\n","- `observation` the observation at the beginning of the step (before acting!)\n","- `action` the action randomly sampled\n","- `current_reward` the reward received after the action was performed\n","\n","After you generate this data, it is recommended that you compute a column (such as `total_reward`, that is the total reward for the entire episode).\n","\n","At the end of the data gathering, you should be able to use pandas (or similar) to calculate the average total reward *per episode* of the random agent. The average score should be 1-2%, meaning that the agent very rarely wins.\n","\n","\n","## Hints\n","\n","- `initial_observation = env.reset()` starts a new episode and returns the initial observation.\n","- `new_observation, reward, done, info, probabilities = env.step(new_action)` executes one action and returns the following observation. You may look at the documentation for the step method if you are curious about what it does. \n","- `done != True` until the game is finished.\n","- we are trying to maximize the reward *per episode*. Our first game gives 0 reward unless the agent travels to the goal.\n","- `env.action_space.n` gives the number of possible actions in the environment. `env.action_space.sample()` allows the agent to randomly sample an action.\n","- `env.observation_space.n` gives the number of possible states in the environment. \n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["env = gym.make('FrozenLake-v1')\n","\n","num_episodes = 40000\n","\n","life_memory = []\n","for i in range(num_episodes):\n","    \n","    # start a new episode and record all the memories\n","    old_observation = env.reset()\n","    done = False\n","    tot_reward = 0\n","    ep_memory = []\n","    while not done:\n","        new_action = env.action_space.sample()\n","        observation, reward, done, info, probs = env.step(new_action)\n","        tot_reward += reward\n","        \n","        ep_memory.append({\n","            \"observation\": old_observation,\n","            \"action\": new_action,\n","            \"reward\": reward,\n","            \"episode\": i\n","            #,\"probs\": probs\n","        })\n","        old_observation = observation\n","        \n","    # incorporate total reward\n","    num_steps = len(ep_memory)\n","    for i, ep_mem in enumerate(ep_memory):\n","        ep_mem[\"tot_reward\"] = tot_reward\n","        ep_mem[\"decay_reward\"] = i*tot_reward/num_steps\n","        \n","    life_memory.extend(ep_memory)\n","    \n","memory_df = pandas.DataFrame(life_memory)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>action</th>\n","      <th>reward</th>\n","      <th>episode</th>\n","      <th>tot_reward</th>\n","      <th>decay_reward</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>305591.000000</td>\n","      <td>305591.000000</td>\n","      <td>305591.000000</td>\n","      <td>305591.000000</td>\n","      <td>305591.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>1.500607</td>\n","      <td>0.001777</td>\n","      <td>19999.038159</td>\n","      <td>0.023914</td>\n","      <td>0.011069</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1.118251</td>\n","      <td>0.042116</td>\n","      <td>11529.214495</td>\n","      <td>0.152783</td>\n","      <td>0.083585</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>10071.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>19965.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>29961.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","      <td>39999.000000</td>\n","      <td>1.000000</td>\n","      <td>0.975610</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              action         reward        episode     tot_reward  \\\n","count  305591.000000  305591.000000  305591.000000  305591.000000   \n","mean        1.500607       0.001777   19999.038159       0.023914   \n","std         1.118251       0.042116   11529.214495       0.152783   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         1.000000       0.000000   10071.000000       0.000000   \n","50%         2.000000       0.000000   19965.000000       0.000000   \n","75%         3.000000       0.000000   29961.000000       0.000000   \n","max         3.000000       1.000000   39999.000000       1.000000   \n","\n","        decay_reward  \n","count  305591.000000  \n","mean        0.011069  \n","std         0.083585  \n","min         0.000000  \n","25%         0.000000  \n","50%         0.000000  \n","75%         0.000000  \n","max         0.975610  "]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["memory_df.describe()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["(305591, 6)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["memory_df.shape"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>observation</th>\n","      <th>action</th>\n","      <th>reward</th>\n","      <th>episode</th>\n","      <th>tot_reward</th>\n","      <th>decay_reward</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>791</th>\n","      <td>(0, {'prob': 1})</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>792</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.076923</td>\n","    </tr>\n","    <tr>\n","      <th>793</th>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.153846</td>\n","    </tr>\n","    <tr>\n","      <th>794</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.230769</td>\n","    </tr>\n","    <tr>\n","      <th>795</th>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.307692</td>\n","    </tr>\n","    <tr>\n","      <th>796</th>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.384615</td>\n","    </tr>\n","    <tr>\n","      <th>797</th>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.461538</td>\n","    </tr>\n","    <tr>\n","      <th>798</th>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.538462</td>\n","    </tr>\n","    <tr>\n","      <th>799</th>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.615385</td>\n","    </tr>\n","    <tr>\n","      <th>800</th>\n","      <td>13</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.692308</td>\n","    </tr>\n","    <tr>\n","      <th>801</th>\n","      <td>14</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.769231</td>\n","    </tr>\n","    <tr>\n","      <th>802</th>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.846154</td>\n","    </tr>\n","    <tr>\n","      <th>803</th>\n","      <td>14</td>\n","      <td>3</td>\n","      <td>1.0</td>\n","      <td>106</td>\n","      <td>1.0</td>\n","      <td>0.923077</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          observation  action  reward  episode  tot_reward  decay_reward\n","791  (0, {'prob': 1})       1     0.0      106         1.0      0.000000\n","792                 4       0     0.0      106         1.0      0.076923\n","793                 4       3     0.0      106         1.0      0.153846\n","794                 4       1     0.0      106         1.0      0.230769\n","795                 8       1     0.0      106         1.0      0.307692\n","796                 8       1     0.0      106         1.0      0.384615\n","797                 9       1     0.0      106         1.0      0.461538\n","798                10       1     0.0      106         1.0      0.538462\n","799                 9       1     0.0      106         1.0      0.615385\n","800                13       3     0.0      106         1.0      0.692308\n","801                14       3     0.0      106         1.0      0.769231\n","802                10       2     0.0      106         1.0      0.846154\n","803                14       3     1.0      106         1.0      0.923077"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["memory_df[memory_df.episode == 106]"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/plain":["0.013575"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["memory_df.groupby(\"episode\").reward.sum().mean()"]},{"cell_type":"markdown","metadata":{},"source":["# Step 2: Predict\n","\n","Now that you have a bunch of data put it into a format that you can model. The goal here is to guide the behavior of our agent. Our agent will be given an observation and need to decide between the possible actions given that observation and the prediction of the model. \n","\n","Remember, you're a data scientist! Be creative. \n","\n","It might be helpful to work backwards. Ultimately, you will write something like:\n","\n","```\n","def convert_to_row(obs, act):\n","    # expertly written code\n","    return row_of_obs_act\n","    \n","rows = [convert_to_row(current_obs, act) for act in possible_actions]\n","\n","pred_outcome = model.predict(rows)\n","```\n","\n","So, you will need to design a quantity that you can ask your model to predict for every possible action-observation pair. Think a bit about what this quantity should be. Should the model try to predict the immediate reward for each action? If so, how would it know where to go at the beginning of each episode when all moves give zero reward but when some moves bring it closer to the goal than others. \n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"setting an array element with a sequence.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'tuple'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\kymno\\Documents\\GitFolders\\ibm_ml_course\\course5_DeepLearning\\05i_DEMO_Reinforcement.ipynb Cell 24\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39mmemory_df\u001b[39m.\u001b[39mreward \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m\u001b[39m*\u001b[39mmemory_df\u001b[39m.\u001b[39mdecay_reward \u001b[39m+\u001b[39m memory_df\u001b[39m.\u001b[39mtot_reward\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m x \u001b[39m=\u001b[39m memory_df[[\u001b[39m\"\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maction\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mfit(x, y)\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    349\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, dtype\u001b[39m=\u001b[39mDTYPE\n\u001b[0;32m    350\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1151\u001b[0m     order\u001b[39m=\u001b[39morder,\n\u001b[0;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[0;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39mensure_2d,\n\u001b[0;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39mallow_nd,\n\u001b[0;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39mensure_min_features,\n\u001b[0;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    914\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 915\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype, xp\u001b[39m=\u001b[39mxp)\n\u001b[0;32m    916\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    919\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:1998\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1997\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1998\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(values, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m   1999\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2000\u001b[0m         astype_is_view(values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   2001\u001b[0m         \u001b[39mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2002\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mis_single_block\n\u001b[0;32m   2003\u001b[0m     ):\n\u001b[0;32m   2004\u001b[0m         \u001b[39m# Check if both conversions can be done without a copy\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m         \u001b[39mif\u001b[39;00m astype_is_view(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m astype_is_view(\n\u001b[0;32m   2006\u001b[0m             values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype\n\u001b[0;32m   2007\u001b[0m         ):\n","\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."]}],"source":["from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n","from sklearn.svm import SVR\n","\n","model = ExtraTreesRegressor(n_estimators=50)\n","# model = SVR()\n","y = 0.5*memory_df.reward + 0.1*memory_df.decay_reward + memory_df.tot_reward\n","x = memory_df[[\"observation\", \"action\"]]\n","model.fit(x, y)"]},{"cell_type":"markdown","metadata":{},"source":["# Step 3: Act\n","\n","Now that you have a model that predicts the desired behavior, let's act on it! Modify the code you used to gather data so that you replace the random decision with an intelligent one.\n","\n","We started out winning ~1.5% of the games with the random agent. How well can you do? You should be able to get your model to do at least 10x better (so 15%). Can you get ~50%?\n","\n","If you're having trouble, tune your model. Try different representations of the observation and action spaces. Try different models. \n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"setting an array element with a sequence.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'tuple'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\kymno\\Documents\\GitFolders\\ibm_ml_course\\course5_DeepLearning\\05i_DEMO_Reinforcement.ipynb Cell 26\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m*\u001b[39mmemory_df\u001b[39m.\u001b[39mreward \u001b[39m+\u001b[39m memory_df\u001b[39m.\u001b[39mtot_reward \u001b[39m+\u001b[39m \u001b[39m.1\u001b[39m\u001b[39m*\u001b[39mmemory_df\u001b[39m.\u001b[39mdecay_reward\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x \u001b[39m=\u001b[39m memory_df[[\u001b[39m\"\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maction\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mfit(x, y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m num_episodes \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m random_per \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    349\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, dtype\u001b[39m=\u001b[39mDTYPE\n\u001b[0;32m    350\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1151\u001b[0m     order\u001b[39m=\u001b[39morder,\n\u001b[0;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[0;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39mensure_2d,\n\u001b[0;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39mallow_nd,\n\u001b[0;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39mensure_min_features,\n\u001b[0;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    914\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 915\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype, xp\u001b[39m=\u001b[39mxp)\n\u001b[0;32m    916\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    919\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n","File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:1998\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1997\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1998\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(values, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m   1999\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2000\u001b[0m         astype_is_view(values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   2001\u001b[0m         \u001b[39mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2002\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mis_single_block\n\u001b[0;32m   2003\u001b[0m     ):\n\u001b[0;32m   2004\u001b[0m         \u001b[39m# Check if both conversions can be done without a copy\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m         \u001b[39mif\u001b[39;00m astype_is_view(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m astype_is_view(\n\u001b[0;32m   2006\u001b[0m             values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype\n\u001b[0;32m   2007\u001b[0m         ):\n","\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."]}],"source":["model = RandomForestRegressor()\n","y = 1*memory_df.reward + memory_df.tot_reward + .1*memory_df.decay_reward\n","x = memory_df[[\"observation\", \"action\"]]\n","model.fit(x, y)\n","\n","num_episodes = 500\n","random_per = 0\n","\n","life_memory = []\n","for i in range(num_episodes):\n","    # Start a new episode and record all the memories.\n","    old_observation = env.reset()\n","    done = False\n","    tot_reward = 0\n","    ep_memory = []\n","    while not done:\n","        if np.random.rand() < random_per:\n","            new_action = env.action_space.sample()\n","        else:\n","            pred_in = [[old_observation,i] for i in range(4)]\n","            new_action = np.argmax(model.predict(pred_in))\n","        observation, reward, done, info = env.step(new_action)\n","        tot_reward += reward\n","        \n","        ep_memory.append({\n","            \"observation\": old_observation,\n","            \"action\": new_action,\n","            \"reward\": reward,\n","            \"episode\": i,\n","        })\n","        old_observation = observation\n","        \n","    # incorporate total reward\n","    for ep_mem in ep_memory:\n","        ep_mem[\"tot_reward\"] = tot_reward\n","        \n","    life_memory.extend(ep_memory)\n","    \n","memory_df2 = pandas.DataFrame(life_memory)\n","\n","# rf.fit(memory_df[[\"observation\", \"action\"]], memory_df[\"comb_reward\"])\n","\n","# Score\n","# Much better!\n","memory_df2.groupby(\"episode\").reward.sum().mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y = .1*memory_df.reward + 1*memory_df.decay_reward + 1*memory_df.tot_reward"]},{"cell_type":"markdown","metadata":{},"source":["# Extension: Pole cart\n","\n","If time permits, try your hand at pole cart (`env = gym.make('CartPole-v0')`).\n","\n","Notice that the observation space is quite different. It's no longer discrete--instead we have 4 continuous values. You'll have to store these differently from how you did with Frozenlake.\n","\n","My random actor actually does surprisingly well (avg ~22). But my intelligent agent is able to score ~99. Can you beat me? \n"]},{"cell_type":"markdown","metadata":{},"source":["# Pole cart\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["env = gym.make('CartPole-v1')"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"ename":"IndexError","evalue":"tuple index out of range","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\kymno\\Documents\\GitFolders\\ibm_ml_course\\course5_DeepLearning\\05i_DEMO_Reinforcement.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     observation, reward, done, info, probs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(new_action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     tot_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     ep_memory\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobs0\u001b[39m\u001b[39m\"\u001b[39m: old_observation[\u001b[39m0\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobs1\u001b[39m\u001b[39m\"\u001b[39m: old_observation[\u001b[39m1\u001b[39m],\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobs2\u001b[39m\u001b[39m\"\u001b[39m: old_observation[\u001b[39m2\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobs3\u001b[39m\u001b[39m\"\u001b[39m: old_observation[\u001b[39m3\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maction\u001b[39m\u001b[39m\"\u001b[39m: new_action,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m: reward,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode\u001b[39m\u001b[39m\"\u001b[39m: i,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     })\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     old_observation \u001b[39m=\u001b[39m observation\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kymno/Documents/GitFolders/ibm_ml_course/course5_DeepLearning/05i_DEMO_Reinforcement.ipynb#X40sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# incorporate total reward\u001b[39;00m\n","\u001b[1;31mIndexError\u001b[0m: tuple index out of range"]}],"source":["# now we can build a toy world!\n","num_episodes = 1000\n","\n","life_memory = []\n","for i in range(num_episodes):\n","    \n","    # start a new episode and record all the memories\n","    old_observation = env.reset()\n","    done = False\n","    tot_reward = 0\n","    ep_memory = []\n","    while not done:\n","        new_action = env.action_space.sample()\n","        observation, reward, done, info, probs = env.step(new_action)\n","        tot_reward += reward\n","        \n","        ep_memory.append({\n","            \"obs0\": old_observation[0],\n","            \"obs1\": old_observation[1],\n","            \"obs2\": old_observation[2],\n","            \"obs3\": old_observation[3],\n","            \"action\": new_action,\n","            \"reward\": reward,\n","            \"episode\": i,\n","        })\n","        old_observation = observation\n","        \n","    # incorporate total reward\n","    for ep_mem in ep_memory:\n","        ep_mem[\"tot_reward\"] = tot_reward\n","        \n","    life_memory.extend(ep_memory)\n","    \n","memory_df = pandas.DataFrame(life_memory)\n","\n","memory_df.groupby(\"episode\").reward.sum().mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["memory_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor\n","\n","model = ExtraTreesRegressor(n_estimators=50)\n","\n","memory_df[\"comb_reward\"] = .5*memory_df.reward + memory_df.tot_reward\n","model.fit(memory_df[[\"obs0\", \"obs1\", \"obs2\", \"obs3\", \"action\"]], memory_df.comb_reward)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_episodes = 100\n","random_per = 0\n","\n","life_memory = []\n","for i in range(num_episodes):\n","    \n","    # start a new episode and record all the memories\n","    old_observation = env.reset()\n","    done = False\n","    tot_reward = 0\n","    ep_memory = []\n","    while not done:\n","        \n","        \n","        if np.random.rand() < random_per:\n","            new_action = env.action_space.sample()\n","        else:\n","            pred_in = [list(old_observation)+[i] for i in range(2)]\n","            new_action = np.argmax(model.predict(pred_in))\n","        observation, reward, done, info = env.step(new_action)\n","        tot_reward += reward\n","        \n","        ep_memory.append({\n","            \"obs0\": old_observation[0],\n","            \"obs1\": old_observation[1],\n","            \"obs2\": old_observation[2],\n","            \"obs3\": old_observation[3],\n","            \"action\": new_action,\n","            \"reward\": reward,\n","            \"episode\": i,\n","        })\n","        old_observation = observation\n","        \n","    # incorporate total reward\n","    for ep_mem in ep_memory:\n","        ep_mem[\"tot_reward\"] = tot_reward\n","        \n","    life_memory.extend(ep_memory)\n","    \n","memory_df2 = pandas.DataFrame(life_memory)\n","memory_df2[\"comb_reward\"] = memory_df2.reward + memory_df2.tot_reward\n","\n","# score\n","# much better!\n","memory_df2.groupby(\"episode\").reward.sum().mean()"]},{"cell_type":"markdown","metadata":{},"source":["---\n","### Machine Learning Foundation  2022 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
