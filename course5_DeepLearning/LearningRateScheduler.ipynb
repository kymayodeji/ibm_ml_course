{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduler Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 is a popular image classification dataset used in computer vision and deep learning. It is comprised of 60,000 32Ã—32 pixel colored images of objects belonging to 10 different classes, that is, airplane, automobile, bird, and so on. In this example, we will be using class 3 (cats) and class 5 (dogs) only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "#from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import LearningRateScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32  \n",
    "\n",
    "# training settings\n",
    "batch_size = 64\n",
    "num_classes = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 25s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at cats and dogs\n",
    "train_picks = np.ravel(np.logical_or(y_train==3,y_train==5))  \n",
    "test_picks = np.ravel(np.logical_or(y_test==3,y_test==5))     \n",
    "y_train = np.array(y_train[train_picks]==5,dtype=int)\n",
    "y_test = np.array(y_test[test_picks]==5,dtype=int)\n",
    "X_train = X_train[train_picks]\n",
    "X_test = X_test[test_picks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI: image_data_format() returns the default image data format convention.\n",
    "# based on that, we pick channels_first or channels_last, which is used to define the input shape\"\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will define and return a base model architecture, which we will use with all LR schedulers. This will allow for direct comparison between all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model() : \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN model\n",
    " Uses a constant Learning Rate for Stochastic Gradient Descent model with default momentum and decay rates of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 4)         112       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 8)         296       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 8)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                25104     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25546 (99.79 KB)\n",
      "Trainable params: 25546 (99.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "decay is deprecated in the new Keras optimizer, please check the docstring for valid arguments, or use the legacy optimizer, e.g., tf.keras.optimizers.legacy.SGD.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m \u001b[39m#define SGD optimizer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=4'>5</a>\u001b[0m sgd \u001b[39m=\u001b[39m SGD(lr\u001b[39m=\u001b[39mlearning_rate, momentum\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, decay\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, nesterov\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m# set to default except lr\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=6'>7</a>\u001b[0m \u001b[39m# compile the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X14sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m model1\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mcategorical_crossentropy, optimizer\u001b[39m=\u001b[39msgd, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\sgd.py:114\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[1;34m(self, learning_rate, momentum, nesterov, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, name, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    113\u001b[0m ):\n\u001b[1;32m--> 114\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    115\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[0;32m    116\u001b[0m         weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[0;32m    117\u001b[0m         clipnorm\u001b[39m=\u001b[39mclipnorm,\n\u001b[0;32m    118\u001b[0m         clipvalue\u001b[39m=\u001b[39mclipvalue,\n\u001b[0;32m    119\u001b[0m         global_clipnorm\u001b[39m=\u001b[39mglobal_clipnorm,\n\u001b[0;32m    120\u001b[0m         use_ema\u001b[39m=\u001b[39muse_ema,\n\u001b[0;32m    121\u001b[0m         ema_momentum\u001b[39m=\u001b[39mema_momentum,\n\u001b[0;32m    122\u001b[0m         ema_overwrite_frequency\u001b[39m=\u001b[39mema_overwrite_frequency,\n\u001b[0;32m    123\u001b[0m         jit_compile\u001b[39m=\u001b[39mjit_compile,\n\u001b[0;32m    124\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_learning_rate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_learning_rate(learning_rate)\n\u001b[0;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39m=\u001b[39m momentum\n",
      "File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1084\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, name, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, **kwargs)\u001b[0m\n\u001b[0;32m   1082\u001b[0m mesh \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mmesh\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1083\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mesh \u001b[39m=\u001b[39m mesh\n\u001b[1;32m-> 1084\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m   1085\u001b[0m     name,\n\u001b[0;32m   1086\u001b[0m     weight_decay,\n\u001b[0;32m   1087\u001b[0m     clipnorm,\n\u001b[0;32m   1088\u001b[0m     clipvalue,\n\u001b[0;32m   1089\u001b[0m     global_clipnorm,\n\u001b[0;32m   1090\u001b[0m     use_ema,\n\u001b[0;32m   1091\u001b[0m     ema_momentum,\n\u001b[0;32m   1092\u001b[0m     ema_overwrite_frequency,\n\u001b[0;32m   1093\u001b[0m     jit_compile,\n\u001b[0;32m   1094\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1095\u001b[0m )\n\u001b[0;32m   1096\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distribution_strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1097\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_with_dtensor \u001b[39m=\u001b[39m dtensor_utils\u001b[39m.\u001b[39mrunning_with_dtensor_strategy()\n",
      "File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:106\u001b[0m, in \u001b[0;36m_BaseOptimizer.__init__\u001b[1;34m(self, name, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variables \u001b[39m=\u001b[39m []\n\u001b[0;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_iteration_variable()\n\u001b[1;32m--> 106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_kwargs(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\kymno\\Anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:135\u001b[0m, in \u001b[0;36m_BaseOptimizer._process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m legacy_kwargs:\n\u001b[1;32m--> 135\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    136\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m is deprecated in the new Keras optimizer, please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcheck the docstring for valid arguments, or use the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlegacy optimizer, e.g., \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf.keras.optimizers.legacy.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         )\n\u001b[0;32m    141\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    143\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m is not a valid argument, kwargs should be empty \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m for `optimizer_experimental.Optimizer`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: decay is deprecated in the new Keras optimizer, please check the docstring for valid arguments, or use the legacy optimizer, e.g., tf.keras.optimizers.legacy.SGD."
     ]
    }
   ],
   "source": [
    "model1 = cnn_model()\n",
    "\n",
    "#define SGD optimizer\n",
    "learning_rate = 0.1\n",
    "sgd = SGD(lr=learning_rate, momentum=0.0, decay=0.0, nesterov=False) # set to default except lr\n",
    "\n",
    "# compile the model\n",
    "model1.compile(loss=keras.losses.categorical_crossentropy, optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "history1 = model1.fit(X_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=2,\n",
    "                validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
